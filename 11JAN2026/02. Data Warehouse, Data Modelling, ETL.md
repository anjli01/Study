# 1. Data Warehouse Fundamentals: Core Architectural Paradigms

These revision notes are tailored for an **Amazon Data Engineer I (L4)** phone screen. At this level, Amazon interviewers look for a solid grasp of "why" you choose a specific tool over another and how data flows through a system.

---

### 1. OLTP vs. OLAP: The Functional Split

| Feature | OLTP (Online Transactional Processing) | OLAP (Online Analytical Processing) |
| :--- | :--- | :--- |
| **Primary Goal** | Handle day-to-day business transactions. | Support decision-making and data analysis. |
| **Data Structure** | **Normalized** (3NF) to reduce redundancy. | **Denormalized** (Star/Snowflake) for speed. |
| **Workload** | High volume of fast, small writes/updates. | Large, complex read queries (aggregations). |
| **Storage Style** | Row-based (usually). | **Columnar** (optimized for aggregations). |
| **AWS Example** | Amazon RDS (Postgres, MySQL), Aurora. | Amazon Redshift. |

#### Key L4 Concept: Why Columnar for OLAP?
*   **I/O Efficiency:** If a query only needs `SUM(sales)`, a columnar store only reads the `sales` column from disk, skipping all others.
*   **Compression:** Data in columns is similar (e.g., all dates), leading to higher compression ratios compared to row-based storage.

---

### 2. Data Warehouse (DW) vs. Data Lake (DL)

| Feature | Data Warehouse (Redshift) | Data Lake (S3) |
| :--- | :--- | :--- |
| **Schema Paradigm** | **Schema-on-Write** (Data must fit a predefined structure). | **Schema-on-Read** (Schema is applied when data is pulled). |
| **Data Quality** | High; highly curated, "Single Version of Truth." | Variable; contains raw, "dirty," and processed data. |
| **Flexibility** | Low; changing schema requires migration. | High; store anything (JSON, Parquet, CSV, Images). |
| **Cost** | Higher (Compute + Storage bundled). | Lower (Decoupled storage and compute). |

#### Key L4 Concept: Schema-on-Write vs. Schema-on-Read
*   **Schema-on-Write:** Ensures data integrity at the door. If data doesn't match the table definition, the load fails. Great for BI dashboards.
*   **Schema-on-Read:** Allows for "Data Exploration." You can store data now and figure out how to parse it later using tools like AWS Glue or Athena.

---

### 3. The "Lakehouse" Architecture (Modern DE Paradigm)

Amazon frequently utilizes a **Lakehouse** approach, moving away from "Siloed" warehouses.

*   **The Flow:** 
    1.  **Ingestion:** Raw data lands in **S3** (Data Lake).
    2.  **Processing:** AWS Glue (Spark) or EMR cleans and transforms data.
    3.  **Storage:** Cleaned data is stored back in S3 in an optimized format (**Parquet**).
    4.  **Consumption:** 
        *   **Redshift Spectrum:** Query data directly in S3 without loading it into Redshift.
        *   **Redshift:** High-performance subsets of data are loaded into Redshift local storage for low-latency reporting.

#### Why Parquet?
*   It is a **columnar** file format.
*   Supports **predicate pushdown** (filtering data at the storage level rather than loading everything into memory).

---

### 4. Data Modeling Fundamentals (L4 Must-Knows)

In an L4 phone screen, you may be asked how to organize tables in a warehouse.

*   **Star Schema:**
    *   **Fact Tables:** Contain quantitative metrics (e.g., `price`, `quantity`) and Foreign Keys. Usually very large.
    *   **Dimension Tables:** Contain descriptive attributes (e.g., `product_name`, `store_location`). Smaller, joined to facts.
    *   *Benefit:* Simplest for BI tools to query; requires fewer joins than normalized schemas.

*   **Snowflake Schema:**
    *   Dimensions are normalized (e.g., a `Product` dimension links to a `Category` dimension table).
    *   *Benefit:* Saves storage, but increases query complexity (more joins).

---

### 5. Potential Phone Screen Questions (Quick Fire)

*   **Q: When would you use S3 instead of Redshift?**
    *   *A:* Use S3 for massive volumes of raw/unstructured data, or when you need a low-cost "holding area" before processing. Use Redshift when you need high-speed SQL performance for complex BI queries.
*   **Q: What is a "Distribution Key" in Redshift? (Amazon Specific)**
    *   *A:* It determines how data is partitioned across compute nodes. Good choices prevent "Data Skew" (where one node does all the work).
*   **Q: How do you handle a scenario where the schema of your source data changes frequently?**
    *   *A:* Use a **Data Lake** approach with Schema-on-Read. Store the data in S3 and use a crawler (like AWS Glue) to update the metadata catalog.

# 2. Amazon Redshift: Fundamental Architecture and Tuning

This revision guide is tailored for an Amazon L4 Data Engineer candidate. At this level, focus on **how** these features work and **why** you would choose one over the other in a real-world scenario.

---

## Amazon Redshift Revision Notes (L4 Screening)

### 1. Fundamental Architecture
Redshift is a **fully managed, petabyte-scale Data Warehouse** optimized for **OLAP** (Online Analytical Processing).

#### Massively Parallel Processing (MPP)
*   **Leader Node:** Receives queries, parses them, creates execution plans, and aggregates results from compute nodes.
*   **Compute Nodes:** Execute the compiled code. They consist of **node slices**, where each slice processes a portion of the data.
*   **Key Benefit:** Performance scales linearly by adding more nodes.

#### Columnar Storage vs. Row-Based
| Feature | Row-Based (OLTP - e.g., PostgreSQL) | Columnar (OLAP - Redshift) |
| :--- | :--- | :--- |
| **Storage Layout** | Data stored sequentially by row. | Data stored sequentially by column. |
| **I/O Efficiency** | Reads entire rows even if only 2 columns are needed. | Reads only the specific columns requested. |
| **Compression** | Harder to compress (mixed data types). | High compression (same data type per block). |
| **Best Use Case** | Small, frequent CRUD operations. | Large-scale aggregations (SUM, AVG) and Joins. |

---

### 2. Distribution Styles
Distribution styles determine how data is spread across compute nodes to minimize **Data Shuffle** (network traffic between nodes), which is the biggest bottleneck in MPP.

| Style | How it Works | Best Use Case | Pros/Cons |
| :--- | :--- | :--- | :--- |
| **EVEN** | Round-robin distribution. | Tables not involved in joins; no clear join key. | **Pros:** Perfectly balanced. **Cons:** High shuffle during joins. |
| **KEY** | Rows with the same value in a column go to the same node. | Large tables frequently joined together. | **Pros:** Colocates data for fast joins. **Cons:** Risk of **Data Skew** if keys aren't uniform. |
| **ALL** | A full copy of the table is placed on every node. | Small dimension tables (< 2-3 million rows). | **Pros:** Zero shuffle joins. **Cons:** High storage cost; slow updates/inserts. |
| **AUTO** | Redshift decides based on table size. | Default for new tables. | Switches from ALL to EVEN as the table grows. |

---

### 3. Sort Keys
Sort keys determine the physical order of data on the disk. Redshift uses **Zone Maps** (metadata storing Min/Max values for each block) to skip irrelevant data.

*   **Compound Sort Key (Default):**
    *   Functions like a composite index in SQL. Order matters (e.g., `(Year, Month, Day)`).
    *   **Use Case:** Best for filters (`WHERE`) or joins on the prefix columns.
*   **Interleaved Sort Key:**
    *   Gives equal weight to all columns in the key.
    *   **Use Case:** Complex queries where you filter by different columns interchangeably. (Note: These are rarely used now compared to Compound).

---

### 4. Data Loading: The COPY Command
For L4 candidates, knowing that `INSERT` is slow for bulk loads is critical. **Always use `COPY` from S3.**

#### Best Practices for `COPY`:
*   **Parallelism:** Split your data into multiple files. The number of files should be a **multiple of the number of slices** in your cluster.
*   **Compression:** Gzip or Bzip2 your files to reduce S3-to-Redshift transfer time.
*   **Manifest Files:** Use a JSON manifest file to ensure the command loads the specific files you want, preventing duplicates or missing data.
*   **Column Mapping:** Use the `JSON 'auto'` or specific mapping if the source file format differs from the table schema.
*   **Workload Management (WLM):** Large `COPY` jobs should be assigned to specific queues to avoid blocking interactive user queries.

---

### 5. Maintenance & Tuning Tips (Quick Hits)
*   **VACUUM:** Reclaims space and resorts data after heavy deletes/updates. (Redshift now performs "Auto Vacuum" in the background).
*   **ANALYZE:** Updates statistics for the query optimizer. Crucial for the Leader Node to pick the best execution plan.
*   **Data Skew:** If one node has 90% of the data (due to poor KEY distribution), the whole cluster waits for that one node. Monitor via `SVV_TABLE_INFO`.
*   **Redshift Spectrum:** Allows querying data directly from S3 (Parquet/ORC) without loading it into Redshift disks. Great for "Cold Data."

---

### Possible "Phone Screen" Scenarios
*   **Q: My query is slow, and I see high "Network Traffic" in the plan. Why?**
    *   **A:** Likely a "Broadcast" or "Shuffle" move. I would check the **Distribution Styles** of the joining tables to see if they can be colocated via KEY distribution.
*   **Q: Why shouldn't I use a single 10GB file for a COPY command?**
    *   **A:** Only one slice will process that file while others sit idle. I should split it into multiple smaller files to utilize the **MPP architecture**.


# 3. Foundational Data Modeling

This revision guide is tailored for the **Amazon Data Engineer I (L4)** phone screen, focusing on the specific data modeling topics you provided.

---

## Data Modeling Revision Notes (Amazon L4)

### 1. Core Concepts: Fact vs. Dimension Tables
At the L4 level, you must distinguish between "what happened" (Facts) and "the context" (Dimensions).

| Feature | Fact Tables | Dimension Tables |
| :--- | :--- | :--- |
| **Contents** | Quantitative measures/metrics (e.g., price, quantity). | Descriptive attributes (e.g., color, city, name). |
| **Keys** | Composed of Foreign Keys (FK) from dimensions. | Usually has a Primary Key (PK) / Surrogate Key. |
| **Grain** | Defines the level of detail for a single row. | Provides context for the fact's grain. |
| **Shape** | Usually "Deep" (many rows, few columns). | Usually "Wide" (fewer rows, many columns). |
| **Example** | `sales_amount`, `order_timestamp`. | `product_category`, `customer_email`. |

---

### 2. Star Schema vs. Snowflake Schema
Amazon interviewers often ask for the trade-offs between these two, especially regarding query performance vs. storage efficiency.

| Feature | Star Schema | Snowflake Schema |
| :--- | :--- | :--- |
| **Normalization** | Denormalized (Dimensions are flat). | Normalized (Dimensions are split into sub-tables). |
| **Joins** | Fewer joins (Fact $\leftrightarrow$ Dim). | More joins (Fact $\leftrightarrow$ Dim $\leftrightarrow$ Sub-Dim). |
| **Query Speed** | Faster (Simpler execution plans). | Slower (Complex joins). |
| **Maintenance** | Higher redundancy, simpler to understand. | Lower redundancy, harder to maintain. |
| **Best Use Case** | **General OLAP / Redshift** (Preferred). | When storage is a major constraint (rare now). |

---

### 3. Slowly Changing Dimensions (SCD)
For L4 roles, focus heavily on **SCD Type 2**, as it is the industry standard for maintaining history.

*   **SCD Type 0:** Fixed (e.g., Date of Birth). Never changes.
*   **SCD Type 1:** Overwrite. No history kept.
*   **SCD Type 2:** **Add a New Row.** Full history is preserved using validity markers.

#### Implementation of SCD Type 2
To track a customerâ€™s address change, your dimension table needs:
*   **Surrogate Key:** A unique integer (e.g., `cust_dim_id`) because the natural ID (`customer_id`) will now repeat.
*   **Effective Start Date:** When the record became active.
*   **End Date:** When the record was superseded (set to `9999-12-31` or `NULL` if current).
*   **Current Flag:** Boolean/String (`Y/N`) for quick filtering of the active record.

---

### 4. Practical Scenario: Tracking Customer Address Changes
*Interview Question: "How would you design a schema to track a customer who moves from Seattle to NYC?"*

#### Approach:
1.  **Requirement:** We need to know where the customer lived at the time of a specific purchase.
2.  **Solution:** Use **SCD Type 2** in the `Dim_Customer` table.
3.  **Schema Example:**

| Cust_SK (PK) | Natural_ID | Name | City | Start_Date | End_Date | Is_Current |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 101 | A-55 | John Doe | Seattle | 2022-01-01 | 2023-05-01 | False |
| 205 | A-55 | John Doe | NYC | 2023-05-02 | 9999-12-31 | True |

*   **Querying for Fact Correlation:** When joining a sale from March 2023 to the dimension, the sale's `order_date` will fall between the Seattle record's `Start_Date` and `End_Date`.

---

### 5. Amazon-Specific Modeling Tips
*   **The Grain:** Always define the "Grain" (the meaning of one row) before you start drawing your tables. (e.g., "One row represents one line item per transaction").
*   **Surrogate Keys:** In Redshift/Distributed environments, mention that Surrogate Keys are used for joins, but you must be mindful of how they affect distribution keys.
*   **Handling Nulls:** In dimensional modeling, avoid `NULL` in Foreign Keys. Use a "Default" row in the dimension table (e.g., `-1` for "Unknown").
*   **Denormalization:** For L4, don't be afraid of denormalization. On modern MPP (Massively Parallel Processing) systems like Amazon Redshift, "flat and wide" is often faster than "highly normalized."

---

### 6. Quick Checklist for the Interview
*   [ ] Can I explain why we use a Star Schema over a Snowflake Schema?
*   [ ] Can I walk through the columns needed for an SCD Type 2 table?
*   [ ] Do I know the difference between a `Primary Key` (Business ID) and a `Surrogate Key` (Data Warehouse ID)?
*   [ ] Can I identify the "Grain" of a retail store model (e.g., Product, Store, Time, Promotion)?


# 4. ETL Engineering and Pipeline Integrity

This revision guide focuses on **ETL Engineering and Pipeline Integrity**, tailored for the Amazon L4 Data Engineer phone screen. At this level, Amazon expects you to demonstrate a solid grasp of how to move data reliably and what to do when things break.

---

### 1. Core Concept: Idempotency
Idempotency is the "Gold Standard" for Amazon pipelines. It ensures that if a job runs multiple times with the same input, the final state of the database remains the same.

*   **Why it matters:** In distributed systems, jobs fail due to network blips or timeouts. If your job isn't idempotent, a retry might result in duplicate data.
*   **How to achieve it:**
    *   **Delete-and-Insert:** Delete existing records for the target time grain (e.g., `WHERE date = '2023-10-01'`) before inserting new ones.
    *   **Upsert (Merge):** Use a unique key to update existing rows and insert new ones.
    *   **Overwriting Partitions:** In S3/Hive-style systems, always use `OVERWRITE` mode on specific partitions rather than `APPEND`.

---

### 2. Loading Strategies: Incremental vs. Full
As an L4, you must know when to use which strategy to balance performance and complexity.

| Feature | Full Load | Incremental Load |
| :--- | :--- | :--- |
| **Definition** | Entire source table is copied every time. | Only new/changed data since the last run is loaded. |
| **Complexity** | Low (simple `SELECT *`). | High (requires tracking state/watermarks). |
| **Scalability** | Poor (fails as data grows). | Excellent (scales with data velocity). |
| **Use Case** | Small dimension tables (e.g., Country Codes). | Large fact tables (e.g., Transactions, Logs). |
| **Methods** | Truncate and Load. | **Watermarking** (using `updated_at` column) or **CDC** (Change Data Capture). |

#### Change Data Capture (CDC)
*   **Query-based:** Polling a `last_modified` timestamp. (Easy to set up, but misses hard deletes).
*   **Log-based:** Reading database transaction logs (e.g., AWS DMS, Debezium). (Highly efficient, captures deletes, but complex).

---

### 3. Data Quality Validation
Amazon values "Data Obsession." Your pipeline should have "Circuit Breakers" to stop bad data from propagating.

*   **Pre-load Checks:** Validate the source data schema and file counts before starting the transform.
*   **In-flight Checks:**
    *   **Null Checks:** Ensure mandatory columns (like `user_id`) aren't null.
    *   **Uniqueness:** Check for duplicate primary keys.
    *   **Distribution/Volume:** Did we suddenly get 50% less data than yesterday? (Anomaly detection).
*   **Post-load Checks:** Row count reconciliation (Source count vs. Target count).

---

### 4. Handling Failures & Backfilling
The phone screen will likely include a scenario: *"Your pipeline failed at 2 AM. What do you do?"*

#### Troubleshooting Steps
1.  **Identify Failure Point:** Check logs (CloudWatch/Airflow/Step Functions) to see if it was a Connection, Permission, or Logic/Data error.
2.  **Isolate the Cause:** Was it a schema change in the source? A cluster OOM (Out of Memory)?
3.  **Fix and Re-run:** Because your pipeline is **idempotent**, you can simply restart it for that specific date.

#### Backfilling
*   **Definition:** Running a pipeline for historical dates to populate a new table or fix old data.
*   **Strategy:** 
    *   Use **Parameterized SQL:** Instead of hardcoding dates, use variables like `START_DATE` and `END_DATE`.
    *   **Concurrency Control:** Ensure backfill jobs don't overwhelm the production database or hit API rate limits.

---

### 5. Amazon-Specific "L4" Interview Tips

*   **Think in "Partitions":** When discussing Redshift or S3, mention partitioning by date. It's the key to both performance and idempotency.
*   **S3 as a Landing Zone:** Always mention a "Staging" area. Don't load raw files directly into production tables. Clean them in a staging environment first.
*   **Mention "Atomic" Operations:** Ensure that a load either completes 100% or doesn't change the target table at all (Atomicity). 
*   **The "Why":** If asked to choose a tool, explain *why* based on scale. (e.g., "I'd use AWS Glue/Spark here because the dataset is multi-terabyte and requires distributed processing.")

### Example Scenario Questions to Practice:
1.  *"How would you handle a source system that doesn't have an `updated_at` column for incremental loads?"* (Answer: CDC or Full Load if small).
2.  *"A late-arriving record from 2 days ago just appeared. How does your pipeline handle it?"* (Answer: Mention re-processing that partition/idempotent upserts).
3.  *"How do you ensure your pipeline doesn't load duplicate data if it's triggered twice by mistake?"* (Answer: Idempotency/Unique constraints).


# 5. Programmatic Foundations and Algorithmic Strategy

This revision guide is tailored for the **Amazon L4 Data Engineer** phone screen, focusing on Python-based programmatic foundations, data manipulation, and scalability.

---

### 1. Python Data Structures: Performance & Use Cases
For DE roles, choosing the right data structure is primarily about optimizing for **lookup speed** and **memory efficiency**.

| Data Structure | Best For... | Time Complexity (Avg) |
| :--- | :--- | :--- |
| **List** | Ordered sequences, iteration, stacks. | Access: $O(1)$, Search: $O(n)$ |
| **Set** | Deduplication, membership testing. | Add/Search: $O(1)$ |
| **Dictionary** | Key-value mapping, frequency counting. | Get/Set: $O(1)$ |
| **Heap (heapq)** | Finding the "Top K" or "Smallest N" elements. | Push/Pop: $O(\log n)$ |

---

### 2. Common Coding Patterns for L4 DE
Amazon L4 coding focuses on **data transformation** rather than complex tree/graph traversal.

#### A. Hash Map / Dictionary Pattern
*   **Use Case:** Finding duplicates, grouping data, or simulating a SQL `JOIN`.
*   **Example:** Finding two IDs that sum to a target or counting occurrences in a log file.
*   **Key Python Tool:** `collections.Counter` or `collections.defaultdict`.

#### B. Array & String Manipulation
*   **Parsing:** Using `.split()`, `.strip()`, and Regex (`re` module) for log file parsing.
*   **Two-Pointer:** Useful for reversing arrays or finding pairs in a sorted list.
*   **Sliding Window:** Useful for processing time-series data or finding sub-sections of logs.

#### C. Deduplication
*   **In-Memory:** `list(set(data))` (Note: This loses order).
*   **Ordered:** `dict.fromkeys(data).keys()`.

---

### 3. Complexity Analysis (Big $O$)
You must be able to state the complexity of your solution immediately after writing it.

| Complexity | Name | Common DE Example |
| :--- | :--- | :--- |
| **$O(1)$** | Constant | Checking if a key exists in a dictionary. |
| **$O(\log n)$** | Logarithmic | Binary search in a sorted ID list. |
| **$O(n)$** | Linear | One pass through a CSV file to find a value. |
| **$O(n \log n)$** | Linearithmic | Standard sorting (`list.sort()` or `sorted()`). |
| **$O(n^2)$** | Quadratic | Nested loops (e.g., comparing every row to every other row). **Avoid this.** |

---

### 4. Scale Considerations: "The Amazon Follow-up"
A common L4 question is: *"Your code works for 1GB, but what if the file is 1TB and you only have 8GB of RAM?"*

#### Strategies for Large-Scale Data:
*   **Generators (`yield`):** Instead of loading an entire list into memory, use a generator to process one line at a time.
    *   *Code:* `for line in open('huge_file.csv'): process(line)`
*   **Chunking:** Use `pandas` (if allowed) with `chunksize` or manually read $N$ lines at a time.
*   **External Sorting:** Sort smaller chunks of data, write them to disk, and then perform a **Merge Sort** on the sorted files.
*   **Hash Partitioning:** Distribute data into smaller "buckets" (files) based on a hash of a key (e.g., `user_id % 10`). Process each bucket individually.

---

### 5. Pythonic DE Best Practices
Amazon interviewers look for "Clean Code" even in phone screens.

*   **List Comprehensions:** Use `[x for x in data if x > 0]` instead of long `for` loops where appropriate.
*   **Type Hinting:** Using `def process_data(records: list) -> dict:` shows seniority.
*   **Error Handling:** Use `try-except` blocks when parsing strings to floats/integers to handle corrupted data.
*   **Built-in Functions:** Leverage `zip()`, `enumerate()`, and `map()` for cleaner data iteration.

---

### 6. Quick Checklist for the Interview
1.  **Clarify the Input:** "Can the input file be empty?" "Is the data sorted?" "Are there null values?"
2.  **Think Aloud:** Explain your logic while typing. Mention that you are using a Hash Map for $O(1)$ lookup.
3.  **Propose a Naive Solution:** If stuck, state the $O(n^2)$ approach first, then immediately explain how to optimize it using a Set or Dictionary.
4.  **Test Cases:** Mentally run a small example (e.g., `[1, 2, 2, 3]`) through your code to check for "off-by-one" errors.


# 6. The AWS Ecosystem for Data Engineering

These revision notes are designed for an Amazon L4 Data Engineer phone screen. At this level, interviewers look for a solid understanding of **why** a service is used, **how** it scales, and **basic cost/performance optimization.**

---

### 1. Amazon S3 (The Data Lake Foundation)
S3 is the primary storage layer for decoupled compute and storage.

*   **Key Concepts:**
    *   **Buckets & Objects:** Global namespace for buckets; objects consist of data, metadata, and a unique key.
    *   **Partitioning (Crucial):** Organizing data by prefix (e.g., `s3://my-bucket/year=2023/month=10/day=27/`). This enables **partition pruning** in Athena/Glue to reduce costs and latency.
    *   **Strong Consistency:** S3 provides strong read-after-write consistency for all applications.
*   **Storage Classes for DEs:**
    *   **S3 Standard:** Frequent access (Active data).
    *   **S3 Intelligent-Tiering:** Automatic cost savings for data with changing access patterns.
    *   **S3 Glacier Instant Retrieval:** Long-term storage but accessible in milliseconds (Good for compliance logs).

| Feature | Best Practice |
| :--- | :--- |
| **File Formats** | Use **Parquet** or **ORC** (columnar) over CSV/JSON for faster analytical queries. |
| **File Sizing** | Avoid "The Small File Problem." Aim for file sizes between **128MB - 512MB**. |
| **Security** | Use **IAM Roles** (for services) and **Bucket Policies** (for resource access). |

---

### 2. AWS Glue (The Serverless ETL)
Glue is a managed ETL service that simplifies data discovery, preparation, and transformation.

*   **The Glue Data Catalog:**
    *   A central repository to store structural and operational metadata (Table definitions, schemas).
    *   Acts as a **Hive Metastore** replacement.
*   **Glue Crawlers:**
    *   Automated scans of S3 to infer schema and create table definitions in the Data Catalog.
    *   Detects new partitions automatically.
*   **Glue ETL (Spark/Python):**
    *   **Dynamic Frames:** An extension of Spark DataFrames that handles schema evolution and "dirty" data better.
    *   **Job Bookmarks:** Persists state to prevent re-processing old data in S3 (enables incremental loads).

---

### 3. AWS Lambda (The Event-Driven Orchestrator)
Lambda allows you to run code without provisioning servers. In DE pipelines, it acts as "the glue between services."

*   **Common DE Use Cases:**
    *   **S3 Triggers:** Automatically start a Glue Job or an Athena query when a `.csv` file is uploaded.
    *   **File Validation:** Check file headers/schema before moving data to a "Clean" bucket.
    *   **API Ingestion:** Small scripts to pull data from a REST API and drop it into S3.
*   **Constraints (Know these limits!):**
    *   **Timeout:** Max 15 minutes.
    *   **Memory:** 128 MB to 10 GB.
    *   **Temporary Storage (`/tmp`):** Up to 10 GB.

---

### 4. Amazon Athena (The Serverless Query Engine)
Athena is an interactive query service that makes it easy to analyze data in S3 using standard SQL.

*   **Architecture:** Based on **Presto/Trino**. It is serverless (no infrastructure to manage).
*   **Pricing:** You pay **$5.00 per TB of data scanned**.
*   **Performance Optimization (Common Interview Topic):**
    *   **Partitioning:** Restrict the amount of data scanned using `WHERE` clauses on partition keys.
    *   **Columnar Formats:** Use Parquet/ORC so Athena only reads the columns required for the query.
    *   **Compression:** Use Snappy or GZIP to reduce file size (lower scan cost).

---

### Summary Table: Which service to use?

| Scenario | Primary Service | Why? |
| :--- | :--- | :--- |
| **Storage for 10PB of raw logs** | **S3** | Highly durable, scalable, and cheap. |
| **Detecting schema of a new dataset** | **Glue Crawler** | Automates metadata creation in the Catalog. |
| **Running a SQL report once a week** | **Athena** | No need for a database; pay only for the query. |
| **Moving a file as soon as it arrives** | **Lambda** | Event-driven and instantaneous. |
| **Heavy Spark transformations (TBs)** | **Glue ETL** | Managed Spark environment scales horizontally. |

---

### Quick Recall for the Interview:
1.  **S3 vs. HDFS:** S3 decouples compute and storage; HDFS couples them. S3 is virtually infinite.
2.  **Athena vs. Redshift:** Athena is for ad-hoc SQL on S3 (serverless). Redshift is for high-performance data warehousing and complex joins (provisioned clusters).
3.  **Partition Pruning:** If you partition by `date`, and query `WHERE date = '2023-01-01'`, Athena skips all other folders. This is the #1 way to save money and time.
