# 1. Relational Fundamentals and Set Logic

This cheat sheet is designed for an **Amazon L4 Data Engineer** candidate. At this level, Amazon expects you to not just write queries, but to understand the **logical flow** and **performance implications** of your SQL.

---

### 1. Join Mechanics & Logic
Understanding how data combines is critical for Amazon's massive datasets (Redshift/Spectrum).

| Join Type | Result Set | Use Case (Amazon Example) |
| :--- | :--- | :--- |
| **INNER JOIN** | Only matching records in both tables. | Orders that have a confirmed shipment. |
| **LEFT JOIN** | All records from Left + matching from Right. | Identifying "Orphaned" records (e.g., FCs with 0 shipments). |
| **FULL OUTER** | All records from both tables. | Consolidating inventory from two different systems. |
| **CROSS JOIN** | Cartesian product (A x B). | Generating a row for every date/warehouse combination. |

*   **Identifying "Orphans":** To find items that have **never** been ordered:
    ```sql
    SELECT p.product_id
    FROM products p
    LEFT JOIN orders o ON p.product_id = o.product_id
    WHERE o.order_id IS NULL; -- The 'Anti-Join' pattern
    ```

---

### 2. Advanced Join Scenarios
L4 candidates are often tested on their ability to handle complex relational patterns.

*   **Self-Joins:**
    *   *Use Case:* Finding duplicate records or sequential events.
    *   *Example:* Finding orders placed by the same customer within 1 hour of each other.
    ```sql
    SELECT a.order_id, b.order_id
    FROM orders a
    JOIN orders b ON a.customer_id = b.customer_id 
      AND a.order_id <> b.order_id
    WHERE b.order_time BETWEEN a.order_time AND a.order_time + interval '1 hour';
    ```
*   **Anti-Joins (Negative Filtering):**
    *   *Pattern:* `NOT EXISTS` or `LEFT JOIN ... WHERE ... IS NULL`.
    *   *Performance Tip:* `NOT EXISTS` is generally safer than `NOT IN` if the subquery contains `NULL` values.
*   **Non-Equi Joins:**
    *   Joins using `<`, `>`, or `BETWEEN`.
    *   *Use Case:* Validating if a shipment occurred during a specific Prime Membership window.
    ```sql
    SELECT s.shipment_id
    FROM shipments s
    JOIN members m ON s.user_id = m.user_id
    WHERE s.ship_date BETWEEN m.start_date AND m.end_date;
    ```

---

### 3. Aggregation & Logical Boundaries
Amazon interviewers look for query efficiency (filtering early).

*   **WHERE vs. HAVING:**
    *   **WHERE:** Filters rows **before** the group is formed (Massively improves performance).
    *   **HAVING:** Filters rows **after** the aggregation is calculated.
    *   *Rule of Thumb:* Use `WHERE` for static attributes and `HAVING` only for aggregate results (e.g., `SUM(sales) > 1000`).

*   **Distinct Count Trick:**
    *   `COUNT(DISTINCT column)` is expensive on Redshift. If you just need to know if a count exceeds a threshold, consider using a subquery with `LIMIT`.

---

### 4. NULL Handling & Data Quality
In real-world data, `NULL` is not `0` or an empty string. It is "Unknown."

| Operation | Behavior with NULLs |
| :--- | :--- |
| `COUNT(*)` | Counts every row, including those with NULLs. |
| `COUNT(col)` | Counts only **non-null** values in that column. |
| `SUM(col)` | Ignores NULLs (if all values are NULL, result is NULL). |
| `GROUP BY` | All NULLs are grouped together into a **single row**. |
| `IN (..., NULL)` | Will never return TRUE if the match isn't found, often leading to empty results. |

*   **The "Zero vs Null" Problem:**
    *   When calculating an average (e.g., `AVG(rating)`), `NULL` ratings are excluded from the denominator. If you want to treat missing ratings as zero, use `COALESCE(rating, 0)`.

---

### 5. Quick Revision Table: Set Operations
When combining result sets instead of joining columns.

| Operation | Duplicate Handling | Use Case |
| :--- | :--- | :--- |
| **UNION** | Removes duplicates (Expensive). | Combining two distinct product catalogs. |
| **UNION ALL** | Keeps duplicates (Performant). | Combining daily log files into a master view. |
| **INTERSECT** | Returns only common rows. | Finding customers who use both Prime Video and Music. |
| **EXCEPT** | Returns rows in A but not B. | Finding products in inventory that have zero sales records. |

---

### Final "L4" Tips for the Screen:
1.  **Ask about Granularity:** Before writing SQL, ask: "Is the `orders` table one row per order, or one row per item?"
2.  **Think about Scale:** If the interviewer asks "How would this perform on a billion rows?", mention filtering by a distribution key or date partition in the `WHERE` clause.
3.  **Validate Assumptions:** Always ask how to handle `NULL` values or duplicate records if they aren't mentioned. This shows a high "Bar Raiser" quality.


# 2. Advanced Analytical Functions (Window Functions)

This revision guide is tailored for the **Amazon Data Engineer I (L4)** phone screen. At this level, Amazon expects you to write clean, performant SQL and demonstrate a deep understanding of how window functions manipulate data sets without losing row-level granularity.

---

### 1. Core Syntax Refresher
Every window function follows this structure:
`FUNCTION() OVER (PARTITION BY column ORDER BY column FRAME_CLAUSE)`

*   **PARTITION BY:** Defines the "bucket" or group (similar to GROUP BY but doesn't collapse rows).
*   **ORDER BY:** Defines the sequence within that bucket.
*   **Frame Clause:** Defines the subset of rows to look at (e.g., "the last 3 days").

---

### 2. Ranking Mechanisms
Crucial for "Top N" and deduplication logic.

| Function | Behavior | Result for tied values (e.g., 10, 10, 12) | Use Case |
| :--- | :--- | :--- | :--- |
| `ROW_NUMBER()` | Unique, sequential integer. | 1, 2, 3 | **Deduplication** (Picking one record). |
| `RANK()` | Leaves gaps after ties. | 1, 1, 3 | Competition ranking (Gold, Gold, Bronze). |
| `DENSE_RANK()` | No gaps after ties. | 1, 1, 2 | Distinct tiers (Top 3 highest salaries). |

---

### 3. Top N & Deduplication Patterns
Amazon interviewers often ask for the "most recent status" or "top performers per category."

**The Pattern (Top N per Region):**
```sql
WITH RankedSales AS (
    SELECT 
        customer_id, region, total_spend,
        DENSE_RANK() OVER (PARTITION BY region ORDER BY total_spend DESC) as rnk
    FROM sales
)
SELECT * FROM RankedSales WHERE rnk <= 3;
```

**Deduplication (Picking the latest record):**
*   **Scenario:** An ETL pipeline ingested duplicate `order_id`s. You need the one with the latest `updated_at` timestamp.
*   **Logic:** Use `ROW_NUMBER()`.
```sql
SELECT order_id, status, updated_at
FROM (
    SELECT *, 
           ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY updated_at DESC) as row_num
    FROM raw_orders
) t
WHERE row_num = 1;
```

---

### 4. Positional Functions (LEAD & LAG)
Used to compare a row to its neighbors (previous/next).

*   **`LAG(col, offset, default)`:** Looks at the **previous** row.
*   **`LEAD(col, offset, default)`:** Looks at the **next** row.

**Common Interview Scenario: Month-over-Month (MoM) Growth**
```sql
SELECT 
    month, 
    monthly_sales,
    LAG(monthly_sales) OVER (ORDER BY month) as prev_month_sales,
    (monthly_sales - LAG(monthly_sales) OVER (ORDER BY month)) / LAG(monthly_sales) OVER (ORDER BY month) * 100 as pct_growth
FROM revenue_table;
```

---

### 5. Running Aggregates & Framing
Used for cumulative totals or moving averages.

**Running Total Syntax:**
```sql
SELECT 
    date, daily_sales,
    SUM(daily_sales) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total
FROM sales;
```

**Frame Clause Cheat Sheet:**
| Frame Clause | Meaning |
| :--- | :--- |
| `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` | From the very start of the partition to now (Standard Running Total). |
| `ROWS BETWEEN 3 PRECEDING AND CURRENT ROW` | Includes the current row + previous 3 (4-day Moving Average). |
| `ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING` | Previous row, current row, and next row. |

**The "Ties" Trap (RANGE vs. ROWS):**
*   **`RANGE` (Default):** If two rows have the same `ORDER BY` value (e.g., same date), `SUM()` will add them together simultaneously.
*   **`ROWS`:** Treats every row individually even if the values in the `ORDER BY` column are the same.
*   *Interview Tip:* Always specify `ROWS BETWEEN...` if you want a precise row-by-row cumulative sum.

---

### 6. Quick Comparison: Window vs. Group By

| Feature | `GROUP BY` | Window Functions |
| :--- | :--- | :--- |
| **Output Rows** | Reduces rows (one per group). | Retains all original rows. |
| **Mixing Data** | Cannot mix aggregate and detail. | Can show `order_id` next to `total_category_sales`. |
| **Efficiency** | Better for simple aggregations. | Eliminates the need for expensive self-joins. |

---

### 7. Amazon-Specific Interview Tips
1.  **Readability counts:** Use CTEs (`WITH` clauses) instead of nested subqueries. Amazon interviewers value maintainable code.
2.  **Edge cases:** If asked for "Top 3," ask if they want ties included. (Ties included = `DENSE_RANK`, No ties/random = `ROW_NUMBER`).
3.  **Performance:** If asked how to optimize these, mention that `PARTITION BY` and `ORDER BY` work best when the columns are part of the **Distribution Key** or **Sort Key** (in Redshift) to avoid data shuffling across nodes.

# 3. Sequence Modeling and Gaps & Islands

These revision notes are designed for the **Amazon DE I (L4)** phone screen, where SQL logic and analytical thinking are prioritized.

---

### 1. Gaps and Islands: The "Island" Problem (Streaks)

The goal is to identify groups of **consecutive** records (e.g., "Find all users with a 5-day login streak").

#### The Core Logic: `Date - ROW_NUMBER()`
The "Magic" of the Island problem is that in a consecutive sequence, the difference between the **Value** and the **Row Number** remains constant.

| Date | Row Number | Date - Row Number (Group Key) |
| :--- | :--- | :--- |
| 2023-01-01 | 1 | 2022-12-31 |
| 2023-01-02 | 2 | 2022-12-31 |
| 2023-01-03 | 3 | 2022-12-31 |
| **2023-01-05** | 4 | **2023-01-01** |

*Notice how the "Group Key" changes only when there is a break in the sequence.*

#### Step-by-Step Implementation
1.  **Deduplicate:** Ensure only one entry per user per day (using `DISTINCT`).
2.  **Sequence:** Assign a `ROW_NUMBER()` partitioned by user and ordered by date.
3.  **Calculate Key:** Subtract the `ROW_NUMBER` (as an interval of days) from the date.
4.  **Group:** Group by User and the Calculated Key to count the length of the streak.

#### SQL Template
```sql
WITH unique_logins AS (
    SELECT DISTINCT user_id, login_date FROM user_activity
),
ranked_logins AS (
    SELECT 
        user_id, 
        login_date,
        ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY login_date) as rn
    FROM unique_logins
),
islands AS (
    SELECT 
        user_id,
        -- Subtracting row number from date creates a constant 'anchor' date
        (login_date - INTERVAL '1 day' * rn) as island_id 
    FROM ranked_logins
)
SELECT user_id, COUNT(*) as streak_length
FROM islands
GROUP BY user_id, island_id
HAVING COUNT(*) >= 5;
```

---

### 2. Gap Analysis (Inactivity & Missing Data)

The goal is to identify periods of inactivity or missing IDs in a sequence.

#### The Core Logic: `LAG()` or `LEAD()`
Use Window Functions to bring the "Previous Timestamp" into the "Current Row" to calculate the delta.

| Transaction ID | Previous ID | Gap Detected? |
| :--- | :--- | :--- |
| 101 | NULL | No |
| 102 | 101 | No (Diff = 1) |
| 105 | 102 | **Yes (Diff = 3)** |

#### Typical Use Cases
*   **Sessionization:** Identify when a user has been inactive for > 30 minutes to define a "new session."
*   **Missing IDs:** Identify skipped sequence numbers in an ETL pipeline.

#### SQL Template (Inactivity Gap)
```sql
WITH time_diffs AS (
    SELECT 
        user_id,
        event_timestamp,
        LAG(event_timestamp) OVER(PARTITION BY user_id ORDER BY event_timestamp) as prev_ts
    FROM logs
)
SELECT 
    user_id,
    event_timestamp as gap_end,
    prev_ts as gap_start,
    EXTRACT(EPOCH FROM (event_timestamp - prev_ts))/60 as minutes_inactive
FROM time_diffs
WHERE (event_timestamp - prev_ts) > INTERVAL '30 minutes';
```

---

### 3. Comparison Table: Islands vs. Gaps

| Feature | Islands (Streaks) | Gaps (Inactivity) |
| :--- | :--- | :--- |
| **Objective** | Grouping consecutive items together. | Finding the distance between items. |
| **Primary Tool** | `ROW_NUMBER()` | `LAG()` or `LEAD()` |
| **Key Metric** | `COUNT(*)` (Length of streak) | `timestamp - prev_timestamp` (Duration) |
| **Amazon Context** | "Active sellers for 3+ months." | "Average time between customer orders." |

---

### 4. Preparation Checklist for Phone Screen
*   [ ] **Identify the "Island" immediately:** If the interviewer says "consecutive," "streak," or "back-to-back," think `Date - ROW_NUMBER()`.
*   [ ] **Handle Ties:** In Island problems, use `DENSE_RANK()` if there are multiple entries for the same timestamp, or `DISTINCT` first.
*   [ ] **Think about Window Frames:** Remember that `ORDER BY` inside a window function creates a default frame (`RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW`).
*   [ ] **Edge Cases:**
    *   What if the streak crosses a year boundary? (Date subtraction handles this automatically).
    *   What if there are nulls? (Window functions ignore nulls in calculations but keep them in the result set).


# 4. Business-Centric SQL Scenarios

This guide is tailored for the **Amazon Data Engineer I (L4)** phone screening, focusing on the specific "Applied SQL" patterns used in their retail and logistics operations.

---

### 1. Retention and Churn
Amazon tracks "Customer Stickiness." You need to identify users who return (Retention) vs. those who stop using the service (Churn).

**Core Logic Table:**
| Metric | Definition | SQL Strategy |
| :--- | :--- | :--- |
| **Retention** | Active in Month $N$ AND Month $N+1$ | `INNER JOIN` table on itself where `user_id` matches and `date` is +1 month. |
| **Churn** | Active in Month $N$ BUT NOT in Month $N+1$ | `LEFT JOIN` Month $N$ to $N+1$; filter where `Month N+1.user_id IS NULL`. |

**Key Pattern: Month-over-Month (MoM) Retention**
```sql
WITH monthly_active AS (
    SELECT DISTINCT user_id, 
           DATE_TRUNC('month', activity_date) AS active_month
    FROM user_activity
)
SELECT 
    curr.active_month,
    COUNT(DISTINCT curr.user_id) AS current_users,
    COUNT(DISTINCT next_mo.user_id) AS retained_users,
    ROUND(COUNT(DISTINCT next_mo.user_id)::numeric / COUNT(DISTINCT curr.user_id), 2) AS retention_rate
FROM monthly_active curr
LEFT JOIN monthly_active next_mo 
    ON curr.user_id = next_mo.user_id 
    AND next_mo.active_month = curr.active_month + INTERVAL '1 month'
GROUP BY 1;
```

---

### 2. Cohort Analysis
Used to track the "life cycle" of users based on when they first joined (e.g., "How do users who joined in Black Friday compare to users who joined in July?").

**The 3-Step Process:**
1.  **Find the "Birth Date":** Determine the `MIN(date)` for every user.
2.  **Calculate the Delta:** Join back to all activity and calculate the difference (months/days) between activity and birth.
3.  **Pivot/Aggregate:** Group by the "Birth Cohort" and the "Delta."

**Common Interview Trap:**
*   *Question:* "What is the 3rd-month retention for the January cohort?"
*   *Answer:* You must filter for the January cohort first, then look for activity exactly 3 months later.

**SQL Structure:**
```sql
WITH user_cohorts AS (
    SELECT user_id, MIN(DATE_TRUNC('month', join_date)) AS cohort_month
    FROM users GROUP BY 1
),
activities AS (
    SELECT a.user_id, 
           EXTRACT(MONTH FROM AGE(DATE_TRUNC('month', a.event_date), c.cohort_month)) AS month_number
    FROM activity a
    JOIN user_cohorts c ON a.user_id = c.user_id
)
SELECT cohort_month, month_number, COUNT(DISTINCT user_id)
FROM activities
GROUP BY 1, 2;
```

---

### 3. E-commerce Product Pairs
Amazon frequently asks for "Frequently Purchased Together" (Market Basket Analysis). This identifies recommendation engine logic.

**The "Self-Join" Pattern:**
To find pairs, you must join the `orders` table to itself on the `order_id`.

**Why `p1.product_id < p2.product_id`?**
*   If you use `!=`, you get duplicates: `(Socks, Shoes)` and `(Shoes, Socks)`.
*   If you use `<`, you only get one unique combination in alphabetical/numerical order.

**Example Scenario Table:**
| Order ID | Product |
| :--- | :--- |
| 101 | Kindle |
| 101 | Case |
| 102 | Kindle |

**SQL Implementation:**
```sql
SELECT 
    p1.product_id AS prod_a,
    p2.product_id AS prod_b,
    COUNT(*) AS frequency
FROM order_items p1
JOIN order_items p2 ON p1.order_id = p2.order_id
WHERE p1.product_id < p2.product_id  -- Avoids mirror pairs and self-pairing
GROUP BY 1, 2
ORDER BY frequency DESC
LIMIT 10;
```

---

### 4. Essential "Amazon-Style" SQL Functions
During the phone screen, you will likely need these window functions and clauses:

| Function | Purpose | Common Amazon Use Case |
| :--- | :--- | :--- |
| `ROW_NUMBER()` | Assigns unique ID to rows | Find the *latest* status of a shipment/package. |
| `RANK()` | Ranks items (with gaps) | Find the top 3 selling products per category. |
| `LAG() / LEAD()` | Accesses prev/next row | Calculate the time difference between two consecutive orders. |
| `COALESCE()` | Handles NULLs | Replacing NULL revenue with `0` for calculations. |
| `DENSE_RANK()` | Ranks without gaps | Ranking vendors by volume where ties exist. |

---

### 5. Final Tips for the Phone Screen
*   **Clarify Ambiguity:** If asked for "active users," ask: "Does active mean logged in, or did they have to make a purchase?"
*   **Think in CTEs:** Amazon interviewers prefer `WITH` clauses over nested subqueries. It makes your logic easier to follow over the phone/CoderPad.
*   **Handle Edge Cases:** 
    *   Mention how you handle `NULL` values.
    *   Ask if the data has duplicates before starting.
*   **Redshift Specifics:** Amazon uses Redshift. While mostly standard SQL, mention that for large datasets, you are mindful of `DISTKEY` and `SORTKEY` (though you likely won't have to write them in a screen).

# 5. Data Modeling and Optimization for SQL

This revision guide is tailored for an **Amazon Data Engineer I (L4)** phone screening. At this level, Amazon looks for strong fundamentals, an understanding of trade-offs, and "SQL-aware" design.

---

### 1. Dimensional Modeling
Designing schemas for analytical performance (OLAP) rather than transactional integrity (OLTP).

| Feature | Star Schema | Snowflake Schema |
| :--- | :--- | :--- |
| **Structure** | Fact table surrounded by single-level dimensions. | Fact table surrounded by normalized (multi-level) dimensions. |
| **Normalization** | Denormalized (redundant data). | Normalized (less redundancy). |
| **Performance** | **Higher** (fewer joins, faster reads). | **Lower** (complex joins). |
| **Maintenance** | Harder (updates required in multiple places). | Easier (data integrity is higher). |
| **Use Case** | Most Data Warehousing (Redshift). | Rarely used in modern DW, unless space is a major constraint. |

*   **Fact Tables:** Contain quantitative metrics (e.g., `sale_amount`) and foreign keys to dimensions.
*   **Dimension Tables:** Contain descriptive attributes (e.g., `product_name`, `store_location`).

---

### 2. Slowly Changing Dimensions (SCD) Type 2
Used to track historical changes over time by creating a new record for every change.

**Key Components:**
*   **Surrogate Key:** A unique primary key (e.g., `row_id`) because the natural key (`customer_id`) is no longer unique.
*   **Effective Dates:** `start_date` and `end_date` to define the validity period.
*   **Current Flag:** A boolean or string (`is_current` or `active`) to quickly filter the latest record.

**Example Table: `dim_customer`**
| Row_ID | Customer_ID | Address | Start_Date | End_Date | Is_Current |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 101 | C1 | Seattle | 2022-01-01 | 2023-05-01 | False |
| 504 | C1 | Austin | 2023-05-02 | NULL | True |

---

### 3. SQL Performance & Optimization
Amazon interviews often involve debugging a slow query. Use these concepts:

#### A. Predicate Pushdown
*   **Definition:** Filtering data as early as possible in the execution plan.
*   **Mechanism:** Move `WHERE` clauses into Subqueries or CTEs before the `JOIN`.
*   **Benefit:** Reduces the number of rows the database has to "shuffle" across the network or keep in memory during a join.

#### B. Avoiding `SELECT *`
*   **Columnar Storage (Redshift):** Unlike Postgres/MySQL (Row-based), Redshift stores each column in a separate file.
*   **The Cost:** If you `SELECT *` on a table with 200 columns, Redshift must perform I/O for 200 files. If you select 2 columns, it reads only 2 files.
*   **Impact:** Massive reduction in Disk I/O and execution time.

---

### 4. Amazon Redshift: Dist & Sort Keys
Redshift is a distributed (MPP) database. Performance depends on how data is spread across nodes.

#### Distribution Styles (Data Placement)
| Style | How it works | Best Use Case |
| :--- | :--- | :--- |
| **KEY** | Data is distributed based on a column's value (hash). | For **Joins**. If two tables are joined on `order_id`, use `KEY` on `order_id` for both to avoid "Data Shuffling." |
| **ALL** | A full copy of the table is placed on every node. | For **Small Dimensions** (< 2M rows). Eliminates shuffling entirely. |
| **EVEN** | Round-robin distribution. | When tables don't join often or have no clear key. Good for **Load Balancing**. |
| **AUTO** | Redshift decides based on size. | Default for new tables. |

#### Sort Keys (Data Ordering)
Sort keys define the physical order of data on disk, enabling **Zone Map** skipping (ignoring blocks of data that don't match the filter).

1.  **Compound Sort Key:** (Most common) Orders columns in a specific hierarchy (e.g., `Year > Month > Day`). Best for filters that follow that hierarchy.
2.  **Interleaved Sort Key:** Gives equal weight to all columns in the key. Useful if you filter on different columns independently (e.g., sometimes by `Product`, sometimes by `Region`).

---

### Quick Recall Tips for the Interview
*   **If asked about a slow query:** Mention checking for **Data Skew** (one node doing more work than others) and **Data Shuffling** (network overhead).
*   **If asked about Star vs. Snowflake:** Always favor **Star Schema** for Redshift because Redshift handles joins at scale better when they are "shallow."
*   **If asked about SCD:** Emphasize that Type 2 allows for **"point-in-time" reporting** (i.e., "Where did the customer live when they made this purchase three years ago?").

# 6. Semi-Structured Data and Cleansing

These revision notes are tailored for an L4 Data Engineer candidate, focusing on the specific tools and logic used at Amazon (Redshift, Athena, and Spark).

---

# Amazon DE Interview Revision: Semi-Structured Data & Cleansing

## 1. Semi-Structured Data (JSON)
Amazon relies heavily on JSON for logs (CloudWatch, S3). You must know how to "flatten" this data into relational tables.

### Key Tools & Methods
| Method | Tool | Usage |
| :--- | :--- | :--- |
| **Dot Notation** | Redshift (SUPER) | Accessing nested fields: `user.profile.id` |
| **JSON_PARSE** | Redshift | Converts a string into a `SUPER` data type. |
| **UNNEST** | Athena / Presto | Expands an array into multiple rows. |
| **JSON_EXTRACT_PATH_TEXT** | Redshift | Extracts a value from a JSON string without converting type. |

### Redshift SUPER Type (Modern Approach)
*   **Storage:** Use the `SUPER` data type to store JSON directly.
*   **Parsing:** `SELECT json_parse(raw_json_string) FROM table;`
*   **Filtering:** Use `IS [NOT] NULL` to check if a nested key exists.

### Athena / Presto (JSON Extraction)
*   **Extracting values:** `json_extract_scalar(json_column, '$.user.id')`
*   **Handling Arrays:** Use `CROSS JOIN UNNEST(array_column) AS t(new_col)` to flatten lists.

---

## 2. Data Cleansing Functions
In an L4 interview, you are expected to handle "dirty" data (nulls, zeros, inconsistent casing) gracefully.

### Core Functions
| Function | Purpose | Amazon Use Case Example |
| :--- | :--- | :--- |
| `COALESCE(val1, val2, ...)` | Returns the first non-null value. | Defaulting a `null` location to 'Unknown'. |
| `NULLIF(val1, val2)` | Returns `null` if val1 = val2. | **Preventing division by zero:** `100 / NULLIF(count, 0)`. |
| `CASE WHEN` | Logical branching. | Standardizing state names: `'CA'` vs `'California'`. |
| `TRIM() / LOWER()` | String formatting. | Fixing white spaces or inconsistent casing in user emails. |

### Practical Logic Scenarios
*   **Standardizing Labels:** 
    ```sql
    CASE 
        WHEN status IN ('shipped', 'delivered') THEN 'Completed'
        WHEN status = 'returned' THEN 'Reversed'
        ELSE 'Pending' 
    END AS order_category
    ```
*   **Defaulting Nulls:** `COALESCE(discount_pct, 0)` ensures mathematical operations don't fail or return `null`.

---

## 3. Regular Expressions (RegEx)
Redshift and Athena support POSIX-compatible regular expressions. You should be able to write basic patterns for data validation.

### Common SQL RegEx Functions
*   `REGEXP_REPLACE(string, pattern, replacement)`: Used to scrub data (e.g., removing special characters).
*   `REGEXP_INSTR(string, pattern)`: Returns the starting position of a pattern.
*   `REGEXP_LIKE(string, pattern)`: Used in `WHERE` clauses for filtering.
*   `REGEXP_SUBSTR(string, pattern)`: Extracts a specific portion of a string (e.g., domain from email).

### Common Meta-Characters Table
| Character | Meaning | Example |
| :--- | :--- | :--- |
| `^` | Start of string | `^Amzn` (Starts with Amzn) |
| `$` | End of string | `\.com$` (Ends with .com) |
| `\d` | Any digit (0-9) | `\d{3}` (Exactly 3 digits) |
| `\s` | Whitespace | Removing tabs/newlines |
| `[a-z]` | Range of characters | Any lowercase letter |
| `+` | One or more | `\d+` (One or more digits) |

### Real-world Interview Examples
1.  **Validating Emails:**
    `WHERE REGEXP_LIKE(email, '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$')`
2.  **Cleaning Phone Numbers:**
    `REGEXP_REPLACE(phone_number, '[^0-9]', '')` — *This removes everything that is NOT a digit.*
3.  **Extracting ID from a URL:**
    `REGEXP_SUBSTR(url, 'id=([0-9]+)', 1, 1, 'e')` — *Extracts the numeric ID from a query parameter.*

---

## Quick Revision Tips for the Screening:
*   **Think about Scale:** If asked how to clean 100TB of data, mention using **Redshift Spectrum** or **AWS Glue (Spark)** to avoid moving data.
*   **Performance:** Mention that `REGEXP` is computationally expensive. If a simple `LIKE` or `LEFT/RIGHT` function works, use that first.
*   **Data Quality:** Always mention checking for **NULLs** and **Duplicates** before performing aggregations.

# 7. Strategic Communication in the Interview

This revision guide focuses on the **"Meta-Skills"** of the Amazon interview: how you think, communicate, and design for scale. For an L4 role, Amazon looks for high "Ownership" and the ability to work without constant hand-holding.

---

### 1. The "Clarify Before Code" Framework
Before writing a single line of SQL or Python, you must define the boundaries of the problem. This demonstrates **Dive Deep** and **Insist on High Standards**.

#### Key Clarification Areas
| Category | What to Ask / Confirm | Why it Matters |
| :--- | :--- | :--- |
| **Data Grain** | "Is this one row per transaction, or one row per item in a transaction?" | Prevents double-counting in aggregations. |
| **Unique Keys** | "What is the primary key? Can a user have multiple active records?" | Determines if you need `ROW_NUMBER()` or simple `DISTINCT`. |
| **Null Handling** | "Should missing values be filtered out, or defaulted to 0/Unknown?" | Impacts the accuracy of averages and sums. |
| **Data Freshness** | "Is this a real-time stream or a daily batch load?" | Influences tool choice (Lambda/Flink vs. Glue/Redshift). |
| **Constraints** | "Are there late-arriving dimensions or out-of-order data?" | Determines if you need a "Look-back" window. |

---

### 2. Strategic Communication Protocol
Use this 4-step communication loop during the technical screening:

1.  **Repeat & Validate:** "To make sure I understand, we want to calculate the 30-day retention rate for users who joined in Q1, correct?"
2.  **State Assumptions:** "I’m going to assume the `timestamp` column is in UTC and that `is_active` is a boolean."
3.  **Think Out Loud:** Explain your logic while coding. "I'm using a `LEFT JOIN` here because we want to keep all users, even those who haven't made a purchase."
4.  **Proactive Optimization:** Once the solution is coded, critique it before the interviewer does. "This works for 10k rows, but for 10B rows, I would partition by `event_date` to prune data."

---

### 3. Production Readiness & Scalability
Amazon values code that is **maintainable, readable, and performant**. Use these talking points to justify your technical choices.

#### SQL Production Standards
*   **CTEs vs. Subqueries:** Use CTEs (`WITH` clauses). They improve readability and allow the optimizer to better understand the execution plan.
*   **Filtering Early:** Always mention "Predicate Pushdown." Use `WHERE` clauses as early as possible to reduce the dataset size before joins.
*   **Window Functions:** Use `RANK()`, `LEAD/LAG`, and `SUM() OVER()` instead of complex self-joins for better performance.
*   **Handling Skew:** Mention that if a join key is skewed (e.g., one `null` or `0` key appears in 50% of rows), you would handle it separately to avoid "Hot Partitions."

#### Python/ETL Production Standards
*   **Idempotency:** "I would design this pipeline to be idempotent, meaning if it runs twice on the same data, the result doesn't change (no duplicate inserts)."
*   **Logging & Monitoring:** "I'd implement try-except blocks with specific error logging to catch data type mismatches early."
*   **Modularity:** "I'd wrap this logic into a function to make it unit-testable."

---

### 4. Navigating Ambiguity (The L4 "Grading" Secret)
At L4, you are often given "under-defined" prompts. Use the table below to navigate common ambiguous scenarios.

| The Vague Prompt | The Strategic Response |
| :--- | :--- |
| "Analyze user growth." | "How do we define 'Growth'? Is it new sign-ups, or Daily Active Users (DAU)?" |
| "The data is messy." | "I'll start by checking for duplicates and schema consistency before applying business logic." |
| "Build a dashboard for X." | "Who is the end-user? Is this for a VP (high-level KPIs) or an Analyst (granular data)?" |
| "The query is slow." | "I would check the execution plan, look for broadcast joins, or check if the data is partitioned correctly." |

---

### 5. Quick Checklist for the Phone Screen
*   [ ] **Identify the Grain:** Explicitly state the grain of your result set.
*   [ ] **Corner Cases:** Mention how you handle `NULLs`, `Divide by Zero`, and `Empty Strings`.
*   [ ] **Performance Thinking:** Mention `Indexing`, `Partitioning`, and `Data Distribution` (Distkey/Sortkey in Redshift context).
*   [ ] **Ask "Why":** If the interviewer asks for a specific metric, briefly ask why that metric matters to the business (shows **Business Acumen**).

**Pro-Tip:** Amazon interviewers are taking notes as fast as they can. If you see/hear them stop typing, you are likely stuck in a loop or being too quiet. **Summarize your logic in 2-3 bullet points** to give them time to catch up and "bank" your positive signals.
