# 1. Foundational AWS Architecture: Redshift and S3

This guide is tailored for an Amazon L4 Data Engineer phone screen. At this level, focus on **"The Why"**: why choose one distribution style over another, or why Parquet is better than CSV for cost.

---

### 1. Amazon Redshift (Data Warehouse)

Redshift is a **columnar, MPP (Massively Parallel Processing)** data warehouse.

#### **A. Architecture Basics**
| Component | Primary Responsibility |
| :--- | :--- |
| **Leader Node** | Receives queries, parses SQL, creates execution plans, and aggregates results from compute nodes. Does **not** store data. |
| **Compute Node** | Executes the compiled code. Each node is split into **slices**. Slices process data in parallel. |
| **Columnar Storage** | Stores data by column rather than row. Drastically reduces I/O by only reading columns required by the query. |

#### **B. Distribution Styles**
*Goal: Minimize data movement (shuffling) across nodes during joins.*

| Style | How it works | Best Use Case |
| :--- | :--- | :--- |
| **AUTO** | Redshift starts with ALL, then switches to EVEN or KEY as the table grows. | Default; good for small/unknown workloads. |
| **EVEN** | Data is distributed in a Round-robin fashion. | Large tables that **don't join often**; ensures perfect load balancing. |
| **KEY** | Rows with the same value in a specific column go to the same slice. | **Joining large tables.** Use the join key as the Dist Key to avoid "shuffling." |
| **ALL** | A full copy of the table is placed on every node. | **Small lookup/dimension tables** (< 2-3 MB). Eliminates data movement during joins. |

#### **C. Sort Keys**
*Goal: Optimize query performance via "Block Pruning" (skipping data).*

*   **Compound Sort Key (Default):**
    *   Like a composite index in SQL. Order matters (A, B, C).
    *   **Best for:** Queries with a prefix of the sort keys in the `WHERE` clause or joins.
*   **Interleaved Sort Key:**
    *   Gives equal weight to all columns in the key.
    *   **Best for:** Complex queries that filter on different columns (e.g., filtering by `Date` sometimes and `CustomerID` other times).
    *   *Note:* More overhead during vacuum/load operations.

---

### 2. S3 Data Lake & Storage

S3 is the foundation of the "Modern Data Architecture" (Lake House) at Amazon.

#### **A. S3 Basics**
*   **Object Storage:** Data is stored as objects (Files) in Buckets. Each object has a **Key** (the full path).
*   **Consistency:** S3 provides strong read-after-write consistency for all applications.
*   **Storage Classes:**
    *   **Standard:** High frequency, low latency.
    *   **Infrequent Access (IA):** Lower storage cost, but you pay a "retrieval fee" per GB.
    *   **Glacier/Glacier Deep Archive:** For long-term archiving (minutes to hours retrieval time). Cheap but slow.

#### **B. File Formats: Row vs. Columnar**
| Feature | Row-based (CSV/JSON) | Columnar (Parquet/ORC) |
| :--- | :--- | :--- |
| **Access Pattern** | Good for reading the whole row. | Good for aggregating specific columns. |
| **Compression** | Poor. | Excellent (saves disk space/cost). |
| **Performance** | Slow for analytical queries. | Fast (supports predicate pushdown/metadata). |
| **AWS Integration** | Standard. | **Highly recommended** for Athena, Redshift Spectrum, and Glue. |

#### **C. Partitioning & Pruning**
*   **What it is:** Organizing S3 data into a folder structure like `s3://my-bucket/year=2023/month=10/day=25/`.
*   **Partition Pruning:** When tools like **Athena** or **Redshift Spectrum** query this data, they use the path to skip folders that don't match the `WHERE` clause.
*   **Why it matters:**
    *   **Performance:** Scans much less data.
    *   **Cost:** S3-based query engines (Athena) charge per TB scanned. Less data scanned = lower bill.

---

### Quick Recall Checklist for the Interviewer

*   **Scenario:** "My query is slow even though the cluster is large."
    *   **Answer:** Check for **Data Skew** (one slice has more data than others due to a bad Dist Key) or **Query Plan** (look for "DS_BCAST_INNER" indicating data shuffling).
*   **Scenario:** "We have 100TB of logs in S3 and want to query them cheaply."
    *   **Answer:** Convert logs to **Parquet**, use **Partitioning** by date, and query via **Amazon Athena**.
*   **Scenario:** "When should I use Redshift vs. Athena?"
    *   **Answer:** Use **Redshift** for complex, high-performance joins and structured data warehousing. Use **Athena** for ad-hoc queries on S3, log analysis, and "pay-per-query" flexibility.

---

### Phone Screen Tips:
*   **STAR Method:** For any "tell me about a time" questions, use Situation, Task, Action, Result.
*   **Scalability:** Mention that you design for "scaling out" (adding more nodes) rather than "scaling up" (bigger nodes).
*   **Cost-Consciousness:** At Amazon, a good Data Engineer is also a cost-efficient one. Mentioning S3 lifecycle policies or Parquet compression scores points.


# 2. Pipeline Orchestration and ETL Concepts

This revision guide is tailored for an Amazon L4 Data Engineer phone screen, focusing on high-level concepts, architectural trade-offs, and Amazon-specific terminology.

---

## Pipeline Orchestration

Orchestration is the "brain" of your data platform. Amazon looks for your ability to handle failures and manage dependencies.

#### 1. Apache Airflow (The Industry Standard)
*   **DAG (Directed Acyclic Graph):** A collection of all tasks you want to run, organized in a way that reflects their relationships and dependencies.
*   **Operators:** The building blocks.
    *   *Action Operators:* Execute code (e.g., `PythonOperator`, `BashOperator`).
    *   *Transfer Operators:* Move data (e.g., `S3ToRedshiftOperator`).
    *   *Sensors:* Wait for an external event (e.g., `S3KeySensor`).
*   **Tasks:** An instantiated operator; a node in the DAG.
*   **XComs:** "Cross-communication." A mechanism to let tasks talk to each other by passing small amounts of metadata.

#### 2. AWS Step Functions (The Serverless Alternative)
*   **State Machine:** Uses ASL (Amazon States Language) to define workflows.
*   **Best for:** Short-lived, event-driven processes or coordinating AWS Lambda functions.
*   **Pros:** Low maintenance (no servers), native integration with AWS services (Glue, Athena, EMR).

#### Airflow vs. Step Functions
| Feature | Apache Airflow (MWAA) | AWS Step Functions |
| :--- | :--- | :--- |
| **Philosophy** | Configuration as Code (Python) | State Machine (JSON/Visual) |
| **Complexity** | High (Powerful, steep learning curve) | Medium (Visual, limited logic) |
| **Scalability** | Scaling workers can be complex | Highly scalable (Serverless) |
| **Integration** | Excellent for hybrid/multi-cloud | Best for AWS-native ecosystems |

---

## Amazon-Specific Interview Tips (L4)

1.  **Focus on "The Why":** Don't just say you used Airflow. Explain *why* you chose it over a cron job or Step Functions.
2.  **Think about Scale:** If the interviewer asks "How do you handle duplicates?", follow up with "How would this work if the dataset was 100TB?"
3.  **Mention "Operational Excellence":** Talk about how you monitor your pipelines (CloudWatch alarms, SNS notifications on failure).
4.  **Ownership:** If a pipeline fails at 2 AM, how do you ensure the business stakeholders are notified and the data is backfilled? (This relates to Amazonâ€™s Leadership Principles).

### Quick Prep Checklist:
*   [ ] Can I explain a time I handled a data quality issue?
*   [ ] Do I know how to make a pipeline idempotent?
*   [ ] Can I explain the difference between a `Full Load` and an `Incremental Load`?
